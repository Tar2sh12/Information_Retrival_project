{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxESEn7t85zD",
        "outputId": "dfc28b94-87ba-4f79-e48a-d06e7c5b47cc"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-rpggyc8yoh",
        "outputId": "94ecc75b-d166-4960-dd8e-fc289c10a0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "['antoni', 'brutu', 'caeser', 'cleopatra', 'merci', 'worser']\n",
            "Document 2:\n",
            "['antoni', 'brutu', 'caeser', 'calpurnia']\n",
            "Document 3:\n",
            "['merci', 'worser']\n",
            "Document 4:\n",
            "['brutu', 'caeser', 'merci', 'worser']\n",
            "Document 5:\n",
            "['caeser', 'merci', 'worser']\n",
            "Document 6:\n",
            "['antoni', 'caeser', 'merci']\n",
            "Document 7:\n",
            "['angel', 'fool', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n",
            "Document 8:\n",
            "['angel', 'fool', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n",
            "Document 9:\n",
            "['angel', 'fool', 'in', 'rush', 'to', 'tread', 'where']\n",
            "Document 10:\n",
            "['fool', 'fear', 'in', 'rush', 'to', 'tread', 'where', 'fool']\n"
          ]
        }
      ],
      "source": [
        "import nltk   #natural laungage toolkit\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "documents = [\n",
        "    \"antony  brutus caeser cleopatra mercy worser\",\n",
        "    \"antony brutus caeser calpurnia \",\n",
        "    \"mercy worser\",\n",
        "    \"brutus caeser mercy worser\",\n",
        "    \"caeser mercy worser\",\n",
        "    \"antony caeser mercy \",\n",
        "    \"angels fools fear in rush to tread where\",\n",
        "    \"angels fools fear in rush to tread where\",\n",
        "    \"angels fools in rush to tread where\",\n",
        "    \"fools fear in rush to tread where fools\"\n",
        "]\n",
        "stemmer = PorterStemmer()\n",
        "def tokenize_and_stem(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    #print(tokens)\n",
        "     # Tokenize\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Stem each token\n",
        "    return stemmed_tokens\n",
        "tokenized_documents = [tokenize_and_stem(doc) for doc in documents]\n",
        "\n",
        "\n",
        "for i, doc in enumerate(tokenized_documents, start=1):\n",
        "  print(f\"Document {i}:\")\n",
        "  print(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "# Build positional index\n",
        "positional_index = defaultdict(lambda: defaultdict(list))  # Term -> {DocID -> [Positions]}\n",
        "counter=0\n",
        "length=0\n",
        "for doc_id, tokens in enumerate(tokenized_documents):\n",
        "     # print(length)\n",
        "\n",
        "      for position, token in enumerate(tokens):\n",
        "          positional_index[token][doc_id].append(position)\n",
        "\n",
        "def phrase_query(query):\n",
        "    result_docs = set()\n",
        "    query_tokens = query.split()\n",
        "    query_len = len(query_tokens)\n",
        "\n",
        "    operator = None\n",
        "    terms1 = []\n",
        "    terms2 = []\n",
        "    bool_not=False\n",
        "    for token in query_tokens: #lw akter mn token\n",
        "        if token == 'not':\n",
        "            operator = token\n",
        "            bool_not=True\n",
        "        else:\n",
        "          if(bool_not==True):\n",
        "            terms2.append(token)\n",
        "          else:\n",
        "            terms1.append(token)\n",
        "    # print(terms1)\n",
        "    # print(terms2)\n",
        "    # print(terms)\n",
        "    if operator == 'not':\n",
        "      s1=listToString(terms1)\n",
        "      s2=listToString(terms2)\n",
        "      set1=phrase_query(s1)\n",
        "      set2=phrase_query(s2)\n",
        "      # print(set1)\n",
        "      # print(set2)\n",
        "      reverse_list=set()\n",
        "      if len(set1)==0 and bool_not==True:\n",
        "        reverse_list=set2\n",
        "        return reverse_list\n",
        "      elif len(set2)==0 and bool_not==True:\n",
        "        reverse_list=set1\n",
        "        return reverse_list\n",
        "      elif bool_not==True:\n",
        "       reverse_list= set(item for item in set1 if item not in set2)\n",
        "       return reverse_list\n",
        "      print(reverse_list)\n",
        "\n",
        "      # result_docs = set(positional_index[terms[0]].keys())\n",
        "      # for term in terms[1:]:\n",
        "      #     result_docs = result_docs.difference(positional_index[term].keys()) #3l4an bn3ks\n",
        "\n",
        "\n",
        "\n",
        "    for doc_id in positional_index[query_tokens[0]]:\n",
        "        positions = positional_index[query_tokens[0]][doc_id]\n",
        "        for pos in positions:\n",
        "            if all(pos + i in positional_index[query_tokens[i]][doc_id] for i in range(1, query_len)):\n",
        "                result_docs.add(doc_id)\n",
        "\n",
        "\n",
        "    return result_docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def listToString(s):\n",
        "\n",
        "    # initialize an empty string\n",
        "    str1 = \"\"\n",
        "\n",
        "    # traverse in the string\n",
        "    for ele in s:\n",
        "        str1 += ele\n",
        "        str1+=\" \"\n",
        "    # return string\n",
        "    return str1\n",
        "\n",
        "#  phrase query\n",
        "phrase = \"caeser not antoni \"\n",
        "listOfQueryToString=tokenize_and_stem(phrase)\n",
        "result = phrase_query(listToString(listOfQueryToString))\n",
        "print(result)\n",
        "docfreq=[]\n",
        "termfreq = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "\n",
        "print(\"Positional Index:\")\n",
        "for term, postings in positional_index.items():\n",
        "    print(f\"{term} -> {dict(postings)}\")\n",
        "    docs_containing_term = len(list(postings.keys()))\n",
        "    docfreq.append(docs_containing_term)\n",
        "    print(f\"Documents containing {term}: {docs_containing_term}\")\n",
        "    for doc_id, positions in postings.items():\n",
        "        term_freq = len(positions)\n",
        "        termfreq[term][doc_id].append(term_freq)\n",
        "        print(f\"Term frequency of '{term}' in document {doc_id}: {term_freq}\")\n",
        "# for i in termfreq:\n",
        "#     print(i)\n",
        "#print(\"\\nDocuments containing the phrase:\", len(phrase))\n",
        "if result:\n",
        "    print(f\"Documents matching the query '{phrase}': {[doc_id + 1 for doc_id in result]}\")\n",
        "else:\n",
        "     print(\"No documents matching the query found.\")\n",
        "print(docfreq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BmC4tmn9HeI",
        "outputId": "c8a7497c-a76d-4042-b0f8-31e28ac6cc52"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{3, 4}\n",
            "Positional Index:\n",
            "antoni -> {0: [0], 1: [0], 5: [0]}\n",
            "Documents containing antoni: 3\n",
            "Term frequency of 'antoni' in document 0: 1\n",
            "Term frequency of 'antoni' in document 1: 1\n",
            "Term frequency of 'antoni' in document 5: 1\n",
            "brutu -> {0: [1], 1: [1], 3: [0]}\n",
            "Documents containing brutu: 3\n",
            "Term frequency of 'brutu' in document 0: 1\n",
            "Term frequency of 'brutu' in document 1: 1\n",
            "Term frequency of 'brutu' in document 3: 1\n",
            "caeser -> {0: [2], 1: [2], 3: [1], 4: [0], 5: [1]}\n",
            "Documents containing caeser: 5\n",
            "Term frequency of 'caeser' in document 0: 1\n",
            "Term frequency of 'caeser' in document 1: 1\n",
            "Term frequency of 'caeser' in document 3: 1\n",
            "Term frequency of 'caeser' in document 4: 1\n",
            "Term frequency of 'caeser' in document 5: 1\n",
            "cleopatra -> {0: [3]}\n",
            "Documents containing cleopatra: 1\n",
            "Term frequency of 'cleopatra' in document 0: 1\n",
            "merci -> {0: [4], 2: [0], 3: [2], 4: [1], 5: [2]}\n",
            "Documents containing merci: 5\n",
            "Term frequency of 'merci' in document 0: 1\n",
            "Term frequency of 'merci' in document 2: 1\n",
            "Term frequency of 'merci' in document 3: 1\n",
            "Term frequency of 'merci' in document 4: 1\n",
            "Term frequency of 'merci' in document 5: 1\n",
            "worser -> {0: [5], 2: [1], 3: [3], 4: [2]}\n",
            "Documents containing worser: 4\n",
            "Term frequency of 'worser' in document 0: 1\n",
            "Term frequency of 'worser' in document 2: 1\n",
            "Term frequency of 'worser' in document 3: 1\n",
            "Term frequency of 'worser' in document 4: 1\n",
            "calpurnia -> {1: [3]}\n",
            "Documents containing calpurnia: 1\n",
            "Term frequency of 'calpurnia' in document 1: 1\n",
            "angel -> {6: [0], 7: [0], 8: [0]}\n",
            "Documents containing angel: 3\n",
            "Term frequency of 'angel' in document 6: 1\n",
            "Term frequency of 'angel' in document 7: 1\n",
            "Term frequency of 'angel' in document 8: 1\n",
            "fool -> {6: [1], 7: [1], 8: [1], 9: [0, 7]}\n",
            "Documents containing fool: 4\n",
            "Term frequency of 'fool' in document 6: 1\n",
            "Term frequency of 'fool' in document 7: 1\n",
            "Term frequency of 'fool' in document 8: 1\n",
            "Term frequency of 'fool' in document 9: 2\n",
            "fear -> {6: [2], 7: [2], 9: [1]}\n",
            "Documents containing fear: 3\n",
            "Term frequency of 'fear' in document 6: 1\n",
            "Term frequency of 'fear' in document 7: 1\n",
            "Term frequency of 'fear' in document 9: 1\n",
            "in -> {6: [3], 7: [3], 8: [2], 9: [2]}\n",
            "Documents containing in: 4\n",
            "Term frequency of 'in' in document 6: 1\n",
            "Term frequency of 'in' in document 7: 1\n",
            "Term frequency of 'in' in document 8: 1\n",
            "Term frequency of 'in' in document 9: 1\n",
            "rush -> {6: [4], 7: [4], 8: [3], 9: [3]}\n",
            "Documents containing rush: 4\n",
            "Term frequency of 'rush' in document 6: 1\n",
            "Term frequency of 'rush' in document 7: 1\n",
            "Term frequency of 'rush' in document 8: 1\n",
            "Term frequency of 'rush' in document 9: 1\n",
            "to -> {6: [5], 7: [5], 8: [4], 9: [4]}\n",
            "Documents containing to: 4\n",
            "Term frequency of 'to' in document 6: 1\n",
            "Term frequency of 'to' in document 7: 1\n",
            "Term frequency of 'to' in document 8: 1\n",
            "Term frequency of 'to' in document 9: 1\n",
            "tread -> {6: [6], 7: [6], 8: [5], 9: [5]}\n",
            "Documents containing tread: 4\n",
            "Term frequency of 'tread' in document 6: 1\n",
            "Term frequency of 'tread' in document 7: 1\n",
            "Term frequency of 'tread' in document 8: 1\n",
            "Term frequency of 'tread' in document 9: 1\n",
            "where -> {6: [7], 7: [7], 8: [6], 9: [6]}\n",
            "Documents containing where: 4\n",
            "Term frequency of 'where' in document 6: 1\n",
            "Term frequency of 'where' in document 7: 1\n",
            "Term frequency of 'where' in document 8: 1\n",
            "Term frequency of 'where' in document 9: 1\n",
            "Documents matching the query 'caeser not antoni ': [4, 5]\n",
            "[3, 3, 5, 1, 5, 4, 1, 3, 4, 3, 4, 4, 4, 4, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install prettytable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT0owgPBBv-Y",
        "outputId": "87f858e6-2ad7-4054-bda4-0fd4e13273ff"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def print_document_term_matrix(tokenized_documents):\n",
        "    # Create a set of unique terms\n",
        "    unique_terms = sorted(set(term for doc in tokenized_documents for term in doc))\n",
        "\n",
        "    # Create a document-term matrix\n",
        "    document_term_matrix = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Fill in the document-term matrix\n",
        "    for i, doc in enumerate(tokenized_documents, start=1):\n",
        "        for term in unique_terms:\n",
        "            if term in doc:\n",
        "                document_term_matrix[term]['d' + str(i)] = 1\n",
        "            else:\n",
        "                document_term_matrix[term]['d' + str(i)] = 0\n",
        "\n",
        "    # Create a PrettyTable instance\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Terms'] + [f'd{i}' for i in range(1, 11)]\n",
        "\n",
        "    # Add rows to the PrettyTable\n",
        "    for term in unique_terms:\n",
        "        row = [term] + [str(document_term_matrix[term]['d' + str(i)]) for i in range(1, 11)]\n",
        "        table.add_row(row)\n",
        "\n",
        "    # return the PrettyTable\n",
        "    print (table)\n"
      ],
      "metadata": {
        "id": "bqtLRtQIBtc8"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QM-RhwgVx8fd"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_document_term_matrix(tokenized_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLVfzkyJDctV",
        "outputId": "dffb8da0-dfbd-447f-c6e9-0adb7ae433aa"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+----+----+----+----+----+----+----+----+-----+\n",
            "|   Terms   | d1 | d2 | d3 | d4 | d5 | d6 | d7 | d8 | d9 | d10 |\n",
            "+-----------+----+----+----+----+----+----+----+----+----+-----+\n",
            "|   angel   | 0  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 1  |  0  |\n",
            "|   antoni  | 1  | 1  | 0  | 0  | 0  | 1  | 0  | 0  | 0  |  0  |\n",
            "|   brutu   | 1  | 1  | 0  | 1  | 0  | 0  | 0  | 0  | 0  |  0  |\n",
            "|   caeser  | 1  | 1  | 0  | 1  | 1  | 1  | 0  | 0  | 0  |  0  |\n",
            "| calpurnia | 0  | 1  | 0  | 0  | 0  | 0  | 0  | 0  | 0  |  0  |\n",
            "| cleopatra | 1  | 0  | 0  | 0  | 0  | 0  | 0  | 0  | 0  |  0  |\n",
            "|    fear   | 0  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 0  |  1  |\n",
            "|    fool   | 0  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 1  |  1  |\n",
            "|     in    | 0  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 1  |  1  |\n",
            "|   merci   | 1  | 0  | 1  | 1  | 1  | 1  | 0  | 0  | 0  |  0  |\n",
            "|    rush   | 0  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 1  |  1  |\n",
            "|     to    | 0  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 1  |  1  |\n",
            "|   tread   | 0  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 1  |  1  |\n",
            "|   where   | 0  | 0  | 0  | 0  | 0  | 0  | 1  | 1  | 1  |  1  |\n",
            "|   worser  | 1  | 0  | 1  | 1  | 1  | 0  | 0  | 0  | 0  |  0  |\n",
            "+-----------+----+----+----+----+----+----+----+----+----+-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def print_weighted_tf_matrix(tokenized_documents):\n",
        "    # Extract unique terms and sort alphabetically\n",
        "    unique_terms = sorted(set(term for doc in tokenized_documents for term in doc))\n",
        "\n",
        "    # Create a document-term matrix with log-normalized TF weights\n",
        "    document_term_matrix = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    # Fill in the document-term matrix with log-normalized TF weights\n",
        "    for term in unique_terms:\n",
        "        for doc_id, doc in enumerate(tokenized_documents, start=1):\n",
        "            term_frequency = doc.count(term)\n",
        "            if term_frequency > 0:\n",
        "                tf_weight = 1 + math.log(term_frequency, 10)\n",
        "            else:\n",
        "                tf_weight = 0\n",
        "            document_term_matrix[term][f'd{doc_id}'] = tf_weight\n",
        "\n",
        "    # Create a PrettyTable instance\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Terms'] + [f'd{i}' for i in range(1, 11)]\n",
        "\n",
        "    # Add rows to the PrettyTable\n",
        "    for term in unique_terms:\n",
        "        row = [term] + [f\"{document_term_matrix[term][f'd{i}']:.2f}\" for i in range(1, 11)]\n",
        "        table.add_row(row)\n",
        "\n",
        "    # Print the PrettyTable\n",
        "    print(table)"
      ],
      "metadata": {
        "id": "hdD6WYLzu3Pj"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_weighted_tf_matrix(tokenized_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPChaGrHEHfP",
        "outputId": "dd3dbf17-2cd0-4d7f-813e-37ce20a2fbfc"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+------+------+------+------+------+------+------+------+------+\n",
            "|   Terms   |  d1  |  d2  |  d3  |  d4  |  d5  |  d6  |  d7  |  d8  |  d9  | d10  |\n",
            "+-----------+------+------+------+------+------+------+------+------+------+------+\n",
            "|   angel   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 1.00 | 0.00 |\n",
            "|   antoni  | 1.00 | 1.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
            "|   brutu   | 1.00 | 1.00 | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
            "|   caeser  | 1.00 | 1.00 | 0.00 | 1.00 | 1.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
            "| calpurnia | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
            "| cleopatra | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
            "|    fear   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 0.00 | 1.00 |\n",
            "|    fool   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 1.00 | 1.30 |\n",
            "|     in    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 1.00 | 1.00 |\n",
            "|   merci   | 1.00 | 0.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
            "|    rush   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 1.00 | 1.00 |\n",
            "|     to    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 1.00 | 1.00 |\n",
            "|   tread   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 1.00 | 1.00 |\n",
            "|   where   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 1.00 | 1.00 |\n",
            "|   worser  | 1.00 | 0.00 | 1.00 | 1.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
            "+-----------+------+------+------+------+------+------+------+------+------+------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def print_df_idf_columns(tokenized_documents):\n",
        "    # Extract unique terms and sort alphabetically\n",
        "    unique_terms = sorted(set(term for doc in tokenized_documents for term in doc))\n",
        "\n",
        "    # Calculate Document Frequency (DF)\n",
        "    document_frequency = defaultdict(int)\n",
        "    for term in unique_terms:\n",
        "        for doc in tokenized_documents:\n",
        "            if term in doc:\n",
        "                document_frequency[term] += 1\n",
        "\n",
        "    # Total number of documents\n",
        "    total_docs = len(tokenized_documents)\n",
        "\n",
        "    # Calculate Inverse Document Frequency (IDF)\n",
        "    inverse_document_frequency = {}\n",
        "    for term, freq in document_frequency.items():\n",
        "        inverse_document_frequency[term] = math.log((total_docs / freq), 10)\n",
        "    # Add rows to the PrettyTable\n",
        "    # Create a PrettyTable instance\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Term', 'DF', 'IDF']\n",
        "    for term in unique_terms:\n",
        "        row = [term, document_frequency[term], f\"{inverse_document_frequency[term]:.9f}\"]\n",
        "        table.add_row(row)\n",
        "\n",
        "    # Print the PrettyTable\n",
        "    print(table)\n",
        "    return inverse_document_frequency\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BG7AHxvo0xPM"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_document_frequency=print_df_idf_columns(tokenized_documents)"
      ],
      "metadata": {
        "id": "kGL1rdtDEsDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47657921-ac34-4922-e348-f223231f5c3c"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+-------------+\n",
            "|    Term   | DF |     IDF     |\n",
            "+-----------+----+-------------+\n",
            "|   angel   | 3  | 0.522878745 |\n",
            "|   antoni  | 3  | 0.522878745 |\n",
            "|   brutu   | 3  | 0.522878745 |\n",
            "|   caeser  | 5  | 0.301029996 |\n",
            "| calpurnia | 1  | 1.000000000 |\n",
            "| cleopatra | 1  | 1.000000000 |\n",
            "|    fear   | 3  | 0.522878745 |\n",
            "|    fool   | 4  | 0.397940009 |\n",
            "|     in    | 4  | 0.397940009 |\n",
            "|   merci   | 5  | 0.301029996 |\n",
            "|    rush   | 4  | 0.397940009 |\n",
            "|     to    | 4  | 0.397940009 |\n",
            "|   tread   | 4  | 0.397940009 |\n",
            "|   where   | 4  | 0.397940009 |\n",
            "|   worser  | 4  | 0.397940009 |\n",
            "+-----------+----+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tf_idf_table(unique_terms, tokenized_documents, inverse_document_frequency):\n",
        "\n",
        "    tf_idf_matrix = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    # Calculate TF-IDF values\n",
        "    for term in unique_terms:\n",
        "        for doc_id, doc in enumerate(tokenized_documents, start=1):\n",
        "            term_frequency = doc.count(term)\n",
        "            if term_frequency > 0:\n",
        "                tf_weight = 1 + math.log(term_frequency)\n",
        "            else:\n",
        "                tf_weight = 0\n",
        "            tf_idf_matrix[term][f'd{doc_id}'] = tf_weight * inverse_document_frequency[term]\n",
        "\n",
        "    return tf_idf_matrix\n",
        "def print_tf_idf_table(tf_idf_matrix):\n",
        "    table = PrettyTable()\n",
        "    header = [''] + [f'd{i}' for i in range(1, 11)]\n",
        "    table.field_names = header\n",
        "\n",
        "    for term, values in tf_idf_matrix.items():\n",
        "        row = [term] + [f\"{values[f'd{i}']:.9f}\" for i in range(1, 11)]\n",
        "        table.add_row(row)\n",
        "\n",
        "    print(table)"
      ],
      "metadata": {
        "id": "-oWy1qlexKQK"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_terms = sorted(set(term for doc in tokenized_documents for term in doc))\n",
        "tf_idf_matrix=calculate_tf_idf_table(unique_terms,tokenized_documents,inverse_document_frequency)\n",
        "print_tf_idf_table(tf_idf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ytDKNfmJ9kt",
        "outputId": "e44a9921-c3c2-495b-c990-ede438dbeaae"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
            "|           |      d1     |      d2     |      d3     |      d4     |      d5     |      d6     |      d7     |      d8     |      d9     |     d10     |\n",
            "+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
            "|   angel   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.522878745 | 0.522878745 | 0.522878745 | 0.000000000 |\n",
            "|   antoni  | 0.522878745 | 0.522878745 | 0.000000000 | 0.000000000 | 0.000000000 | 0.522878745 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "|   brutu   | 0.522878745 | 0.522878745 | 0.000000000 | 0.522878745 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "|   caeser  | 0.301029996 | 0.301029996 | 0.000000000 | 0.301029996 | 0.301029996 | 0.301029996 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "| calpurnia | 0.000000000 | 1.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "| cleopatra | 1.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "|    fear   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.522878745 | 0.522878745 | 0.000000000 | 0.522878745 |\n",
            "|    fool   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.397940009 | 0.397940009 | 0.397940009 | 0.673771004 |\n",
            "|     in    | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.397940009 | 0.397940009 | 0.397940009 | 0.397940009 |\n",
            "|   merci   | 0.301029996 | 0.000000000 | 0.301029996 | 0.301029996 | 0.301029996 | 0.301029996 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "|    rush   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.397940009 | 0.397940009 | 0.397940009 | 0.397940009 |\n",
            "|     to    | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.397940009 | 0.397940009 | 0.397940009 | 0.397940009 |\n",
            "|   tread   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.397940009 | 0.397940009 | 0.397940009 | 0.397940009 |\n",
            "|   where   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.397940009 | 0.397940009 | 0.397940009 | 0.397940009 |\n",
            "|   worser  | 0.397940009 | 0.000000000 | 0.397940009 | 0.397940009 | 0.397940009 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def calculate_document_length(tf_idf_matrix):\n",
        "    doc_lengths = {}\n",
        "\n",
        "    for doc_id in range(1, 11):\n",
        "        length = 0\n",
        "        for term, values in tf_idf_matrix.items():\n",
        "            length += values[f'd{doc_id}'] ** 2  # Squaring each value and summing them up\n",
        "        doc_lengths[f'd{doc_id}'] = math.sqrt(length)  # Taking square root to get the length\n",
        "\n",
        "    return doc_lengths\n",
        "\n",
        "\n",
        "def print_document_lengths(doc_lengths):\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Document', 'Length']\n",
        "\n",
        "    for doc_id, length in doc_lengths.items():\n",
        "        table.add_row([doc_id, f'{length:.9f}'])\n",
        "\n",
        "    print(table)"
      ],
      "metadata": {
        "id": "agiCP3rQONC_"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_lengths = calculate_document_length(tf_idf_matrix)\n",
        "print_document_lengths(doc_lengths)"
      ],
      "metadata": {
        "id": "5g-0uKVsROpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b62dc61-8ac5-409a-ae11-6cad4188ae5d"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "| Document |    Length   |\n",
            "+----------+-------------+\n",
            "|    d1    | 1.373462315 |\n",
            "|    d2    | 1.279618468 |\n",
            "|    d3    | 0.498974257 |\n",
            "|    d4    | 0.782940962 |\n",
            "|    d5    | 0.582747258 |\n",
            "|    d6    | 0.674270197 |\n",
            "|    d7    | 1.223495757 |\n",
            "|    d8    | 1.223495757 |\n",
            "|    d9    | 1.106137281 |\n",
            "|   d10    | 1.232538356 |\n",
            "+----------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def calculate_normalized_tf_idf(unique_terms, tokenized_documents, inverse_document_frequency):\n",
        "\n",
        "\n",
        "    # Calculate document lengths\n",
        "    doc_lengths = calculate_document_length(tf_idf_matrix)\n",
        "\n",
        "    # Normalize TF-IDF values\n",
        "    for term in unique_terms:\n",
        "        for doc_id in range(1, 11):\n",
        "            length = doc_lengths[f'd{doc_id}']\n",
        "            tf_idf_value = tf_idf_matrix[term][f'd{doc_id}']\n",
        "            if length != 0:\n",
        "                tf_idf_matrix[term][f'd{doc_id}'] = tf_idf_value / length\n",
        "            else:\n",
        "                tf_idf_matrix[term][f'd{doc_id}'] = 0.0  # Assign 0 if length is 0\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_normalized_tf_idf(x):\n",
        "    table = PrettyTable()\n",
        "    header = ['Terms'] + [f'd{i}' for i in range(1, 11)]\n",
        "    table.field_names = header\n",
        "\n",
        "    for term, values in tf_idf_matrix.items():\n",
        "        row = [term] + [f\"{values[f'd{i}']:.9f}\" for i in range(1, 11)]\n",
        "        table.add_row(row)\n",
        "\n",
        "    print(table)"
      ],
      "metadata": {
        "id": "lgEDFhkMRKm_"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=calculate_normalized_tf_idf(unique_terms, tokenized_documents, inverse_document_frequency)\n",
        "print_normalized_tf_idf(x)"
      ],
      "metadata": {
        "id": "uCbPDCKWUADP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf12abd6-0bf6-4a4f-dd4d-5f36f93624af"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
            "|   Terms   |      d1     |      d2     |      d3     |      d4     |      d5     |      d6     |      d7     |      d8     |      d9     |     d10     |\n",
            "+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
            "|   angel   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.427364576 | 0.427364576 | 0.472706918 | 0.000000000 |\n",
            "|   antoni  | 0.380701195 | 0.408620818 | 0.000000000 | 0.000000000 | 0.000000000 | 0.775473612 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "|   brutu   | 0.380701195 | 0.408620818 | 0.000000000 | 0.667839302 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "|   caeser  | 0.219176014 | 0.235249806 | 0.000000000 | 0.384486201 | 0.516570419 | 0.446453064 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "| calpurnia | 0.000000000 | 0.781482938 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "| cleopatra | 0.728086959 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "|    fear   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.427364576 | 0.427364576 | 0.000000000 | 0.424229187 |\n",
            "|    fool   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.325248377 | 0.325248377 | 0.359756438 | 0.546653173 |\n",
            "|     in    | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.325248377 | 0.325248377 | 0.359756438 | 0.322862170 |\n",
            "|   merci   | 0.219176014 | 0.000000000 | 0.603297648 | 0.384486201 | 0.516570419 | 0.446453064 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "|    rush   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.325248377 | 0.325248377 | 0.359756438 | 0.322862170 |\n",
            "|     to    | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.325248377 | 0.325248377 | 0.359756438 | 0.322862170 |\n",
            "|   tread   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.325248377 | 0.325248377 | 0.359756438 | 0.322862170 |\n",
            "|   where   | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.325248377 | 0.325248377 | 0.359756438 | 0.322862170 |\n",
            "|   worser  | 0.289734931 | 0.000000000 | 0.797516111 | 0.508263111 | 0.682868950 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 |\n",
            "+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s=[\"antoni\",\"brutu\"]\n",
        "# for term in s:\n",
        "#   print(s.count(term))\n",
        "def boolean_query(query):\n",
        "  whole_strings=query.split()\n",
        "\n",
        "  print(whole_strings)\n",
        "  l1=[]\n",
        "  l2=[]\n",
        "  result=set()\n",
        "  if(len(whole_strings)==1 ):\n",
        "      result=phrase_query(whole_strings[0])\n",
        "  if(len(whole_strings)==0 ):\n",
        "    print(\"please enter valid query\")\n",
        "  l1_check=True\n",
        "  and_bool=True\n",
        "  def intersection_two_list(l1,l2,check):\n",
        "    result1=set()\n",
        "    if  len(l1)>0 and  len(l2)>0 :\n",
        "      l1_str = \" \"\n",
        "      for a in l1:\n",
        "          l1_str = l1_str + ' ' + a\n",
        "\n",
        "      l2_str = \" \"\n",
        "      for a in l2:\n",
        "          l2_str = l2_str + ' ' + a\n",
        "      x=phrase_query(l1_str)\n",
        "      y=phrase_query(l2_str)\n",
        "      if check==True :\n",
        "        result1= x.intersection(y)\n",
        "      else:\n",
        "        result1=y.union(x)\n",
        "\n",
        "      return result1\n",
        "\n",
        "\n",
        "\n",
        "  first_two=True\n",
        "  l3=[]\n",
        "  c=0\n",
        "  for term in whole_strings:\n",
        "    if term != \"and\" and term !=\"or\" and first_two==True:\n",
        "      if l1_check==True :\n",
        "        l1.append(term)\n",
        "      else:\n",
        "        l2.append(term)\n",
        "    elif first_two==True:\n",
        "      if term ==\"and\":\n",
        "        and_bool=True\n",
        "      else:\n",
        "        and_bool=False\n",
        "      l1_check=False\n",
        "    elif first_two==False:\n",
        "\n",
        "      l3.append(term)\n",
        "\n",
        "    if term==\"and\" or term == \"or\":\n",
        "      c=c+1\n",
        "\n",
        "\n",
        "    if len(l1)>0 and  len(l2)>0 and (term ==\"and\" or term ==\"or\"):\n",
        "      first_two=False\n",
        "      if len(result)==0:\n",
        "        result=intersection_two_list(l1,l2,and_bool)\n",
        "\n",
        "      else:\n",
        "        if and_bool==True :\n",
        "          r=intersection_two_list(l1,l2,and_bool)\n",
        "          result= result.intersection(r)\n",
        "        else:\n",
        "          result=result.union(r)\n",
        "      l1.clear()\n",
        "      l2.clear()\n",
        "      if term ==\"and\":\n",
        "        and_bool=True\n",
        "      else:\n",
        "        and_bool=False\n",
        "\n",
        "\n",
        "\n",
        "    elif(whole_strings.index(term)== len(whole_strings)-1 ) and len(l1)>0 and  len(l2)>0 :\n",
        "\n",
        "      if len(result)==0:\n",
        "        result=intersection_two_list(l1,l2,and_bool)\n",
        "      else:\n",
        "        r=set()\n",
        "        if and_bool==True :\n",
        "          r=intersection_two_list(l1,l2,and_bool)\n",
        "          result= result.intersection(r)\n",
        "        else:\n",
        "          result=result.union(r)\n",
        "\n",
        "    elif len(l1)==0 and  len(l2)==0 and first_two==False and (term ==\"and\" or term ==\"or\"):\n",
        "      l3_str = \" \"\n",
        "      for a in l3:\n",
        "          l3_str = l3_str + ' ' + a\n",
        "\n",
        "      x=phrase_query(l3_str)\n",
        "\n",
        "      if and_bool==True :\n",
        "        result= result.intersection(x)\n",
        "      else:\n",
        "        result=result.union(x)\n",
        "      l3.clear()\n",
        "      if term ==\"and\":\n",
        "        and_bool=True\n",
        "      else:\n",
        "        and_bool=False\n",
        "\n",
        "    elif (whole_strings.index(term)== len(whole_strings)-1) and len(l1)==0 and  len(l2)==0:\n",
        "      l3_str = \" \"\n",
        "      for a in l3:\n",
        "          l3_str = l3_str + ' ' + a\n",
        "\n",
        "      x=phrase_query(l3_str)\n",
        "\n",
        "      if and_bool==True :\n",
        "        result= result.intersection(x)\n",
        "      else:\n",
        "        result=result.union(x)\n",
        "  if(c==0):\n",
        "    l3_str=\"\"\n",
        "    for a in whole_strings:\n",
        "          l3_str = l3_str + ' ' + a\n",
        "    result=phrase_query(l3_str)\n",
        "  # print(result)\n",
        "  return result\n",
        "\n",
        "# boolean_query(\"caeser not antoni or in\")"
      ],
      "metadata": {
        "id": "Ko1kS24fBe64"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_weighted_tf(query_tf):\n",
        "  query_tf_weight={}\n",
        "  for term in query_tf:\n",
        "    if query_tf[term] > 0:\n",
        "        query_tf_weight[term]  = 1 + math.log(query_tf[term] , 10)\n",
        "    else:\n",
        "        query_tf_weight[term]  = 0\n",
        "  return query_tf_weight\n"
      ],
      "metadata": {
        "id": "67ch3GckIJtb"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_df_idf_columns_query(tokenized_documents,query):\n",
        "    # Extract unique terms and sort alphabetically\n",
        "    unique_terms =sorted(set(query))\n",
        "\n",
        "\n",
        "\n",
        "    # print(unique_terms)\n",
        "    # Calculate Document Frequency (DF)\n",
        "    document_frequency = defaultdict(int)\n",
        "\n",
        "    for term in unique_terms:\n",
        "        for doc in tokenized_documents:\n",
        "            if term in doc:\n",
        "                document_frequency[term] += 1\n",
        "\n",
        "    # Total number of documents\n",
        "    total_docs = len(tokenized_documents)\n",
        "    # print(document_frequency)\n",
        "    # Calculate Inverse Document Frequency (IDF)\n",
        "    inverse_document_frequency = {}\n",
        "    for term, freq in document_frequency.items():\n",
        "        inverse_document_frequency[term] = math.log((total_docs / freq), 10)\n",
        "    return inverse_document_frequency\n"
      ],
      "metadata": {
        "id": "R-8omeQBPxgz"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_tf_idf(query_idf,query_tf_weight_result):\n",
        "  query_tf_multi_idf={}\n",
        "  for term in query_idf:\n",
        "    query_tf_multi_idf[term]=query_idf[term]*query_tf_weight_result[term]\n",
        "  return query_tf_multi_idf\n"
      ],
      "metadata": {
        "id": "OEyYBM8CVDIa"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ec_length(result_tfxidf):\n",
        "  result_float=0\n",
        "  for  values in result_tfxidf:\n",
        "    result_float += result_tfxidf[values] ** 2  # Squaring each value and summing them up\n",
        "  return math.sqrt(result_float)\n"
      ],
      "metadata": {
        "id": "X5nGobXGZDRk"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalaized_term(result_tfxidf,ec_length_result):\n",
        "  normalaized_map={}\n",
        "  for  values in result_tfxidf:\n",
        "    normalaized_map[values]= result_tfxidf[values]/ ec_length_result\n",
        "  return normalaized_map\n"
      ],
      "metadata": {
        "id": "0KJplxelaX8o"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Query_processing(stemmed_tokenized,printOrNot):\n",
        "  query_tf={}\n",
        "  for term in stemmed_tokenized:\n",
        "    term_frequency = stemmed_tokenized.count(term)\n",
        "    query_tf[term]=term_frequency\n",
        "  if printOrNot:\n",
        "    print(query_tf)\n",
        "  query_tf_weight_result=query_weighted_tf(query_tf)\n",
        "  if printOrNot:\n",
        "   print(query_tf_weight_result)\n",
        "\n",
        "  query_idf=print_df_idf_columns_query(tokenized_documents,stemmed_tokenized)\n",
        "  if printOrNot:\n",
        "   print(query_idf)\n",
        "\n",
        "  result_tfxidf = query_tf_idf(query_idf,query_tf_weight_result)\n",
        "  if printOrNot:\n",
        "   print(result_tfxidf)\n",
        "\n",
        "  ec_length_result=ec_length(result_tfxidf)\n",
        "  print(f'EUCLIDEAN LENGTH {ec_length_result}')\n",
        "\n",
        "  return normalaized_term(result_tfxidf,ec_length_result)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cp9ghfVXsSHF"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def product_query_X_matched_docs_phrase_query(stemmed_tokenized,normalaized_query_result,tokenized_documents):\n",
        "  query_str=\"\"\n",
        "  dif=[0,1,2,3,4,5,6,7,8,9]\n",
        "  for a in stemmed_tokenized:\n",
        "        query_str = query_str + ' ' + a\n",
        "  list_result=[]\n",
        "  set_of_matcehd_docs=boolean_query(query_str)\n",
        "  reverse_list=[]\n",
        "  bool_check=False\n",
        "  if stemmed_tokenized[0]==\"not\":\n",
        "    reverse_list = [item for item in dif if item not in set_of_matcehd_docs]\n",
        "    # print(reverse_list)\n",
        "    bool_check=True\n",
        "  set_to_list = list(set_of_matcehd_docs)\n",
        "  if bool_check==True:\n",
        "    set_to_list=reverse_list\n",
        "  print(set_to_list)\n",
        "  for doc in set_to_list:\n",
        "    list_result.append(Query_processing(tokenized_documents[doc],False))\n",
        "  # print(list_result)\n",
        "  c=0\n",
        "  final_result = defaultdict(lambda: defaultdict(list))\n",
        "  for map in list_result:\n",
        "    for i in normalaized_query_result:\n",
        "      if map.get(i) == None:\n",
        "        final_result[set_to_list[c]][i]=0\n",
        "      else:\n",
        "        final_result[set_to_list[c]][i]=normalaized_query_result[i]*map[i]\n",
        "\n",
        "    c=c+1\n",
        "\n",
        "  map_summation={}\n",
        "  for i in final_result:\n",
        "    print(f'doc {i+1} {final_result[i]}')\n",
        "  for r in final_result:\n",
        "    sum=0\n",
        "    for term in final_result[r]:\n",
        "      sum+=final_result[r][term]\n",
        "    map_summation[r]=sum\n",
        "\n",
        "  return map_summation\n"
      ],
      "metadata": {
        "id": "UIbiDy9mpcFB"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_input=input()\n",
        "query_str_without_anding_oring_not=[]\n",
        "\n",
        "\n",
        "stemmed_tokenized= tokenize_and_stem(query_input)\n",
        "for i in stemmed_tokenized:\n",
        "  if i == \"and\" or i == \"or\" or i == \"not\":\n",
        "    continue\n",
        "  else:\n",
        "    query_str_without_anding_oring_not.append(i)\n",
        "print(query_str_without_anding_oring_not)\n",
        "query_str_without_anding_oring_not.sort()\n",
        "\n",
        "normalaized_query_result= Query_processing(query_str_without_anding_oring_not,True)\n",
        "print(normalaized_query_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZszHdIinuWz",
        "outputId": "b7a8de8c-151f-4d65-8cf4-16e8315300ac"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fools fear not where fool\n",
            "['fool', 'fear', 'where', 'fool']\n",
            "{'fear': 1, 'fool': 2, 'where': 1}\n",
            "{'fear': 1.0, 'fool': 1.3010299956639813, 'where': 1.0}\n",
            "{'fear': 0.5228787452803376, 'fool': 0.3979400086720376, 'where': 0.3979400086720376}\n",
            "{'fear': 0.5228787452803376, 'fool': 0.5177318877571058, 'where': 0.3979400086720376}\n",
            "EUCLIDEAN LENGTH 0.8365433284465175\n",
            "{'fear': 0.6250468176601647, 'fool': 0.6188942881399189, 'where': 0.47569563361532324}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapy=product_query_X_matched_docs_phrase_query(stemmed_tokenized,normalaized_query_result,tokenized_documents)\n",
        "sorted_dict = dict(sorted(mapy.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "for sd in sorted_dict:\n",
        "  print(f'doc{sd+1} = {sorted_dict[sd]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDA_0RObuK2L",
        "outputId": "12f9e023-e92e-448a-a283-5c923bfac0d3"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fool', 'fear', 'not', 'where', 'fool']\n",
            "[6, 7]\n",
            "EUCLIDEAN LENGTH 1.2234957570597818\n",
            "EUCLIDEAN LENGTH 1.2234957570597818\n",
            "doc 7 defaultdict(<class 'list'>, {'fear': 0.2671228681209442, 'fool': 0.20129436245968127, 'where': 0.15471923255462727})\n",
            "doc 8 defaultdict(<class 'list'>, {'fear': 0.2671228681209442, 'fool': 0.20129436245968127, 'where': 0.15471923255462727})\n",
            "doc7 = 0.6231364631352527\n",
            "doc8 = 0.6231364631352527\n"
          ]
        }
      ]
    }
  ]
}